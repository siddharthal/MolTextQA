{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "class ChatmlSpecialTokens(str, Enum):\n",
    "    user = \"<|im_start|>user\"\n",
    "    assistant = \"<|im_start|>assistant\"\n",
    "    system = \"<|im_start|>system\"\n",
    "    eos_token = \"<|im_end|>\"\n",
    "    bos_token = \"<s>\"\n",
    "    pad_token = \"<pad>\"\n",
    "\n",
    "    @classmethod\n",
    "    def list(cls):\n",
    "        return [c.value for c in cls]\n",
    "\n",
    "from peft import PeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace model name and checkpoint path for Llama2-7b\n",
    "\n",
    "model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "checkpoint_path = \"llama3_8b_checkpoint\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_storage=\"uint8\",\n",
    "        )\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model,\n",
    "            quantization_config= bnb_config,\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation= \"eager\",\n",
    "            torch_dtype=\"auto\",\n",
    "        )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model,\n",
    "            pad_token=special_tokens.pad_token.value,\n",
    "            bos_token=special_tokens.bos_token.value,\n",
    "            eos_token=special_tokens.eos_token.value,\n",
    "            additional_special_tokens=special_tokens.list(),\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "\n",
    "tokenizer.chat_template = DEFAULT_CHATML_CHAT_TEMPLATE\n",
    "        # make embedding resizing configurable?\n",
    "model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\n",
    "\n",
    "finetuned_model = PeftModel.from_pretrained(model,\n",
    "                                  checkpoint_path,\n",
    "                                  torch_dtype=\"auto\",\n",
    "                                  is_trainable=False,\n",
    "                                  device_map=\"auto\"\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = \"<|im_start|>user \\nTo the best of your knowledge, given a smiles string and a question, pick the correct option between 1-5. Answer with single integer only. SMILES string: CCC(C)C(NC(=O)C1CCCN1C(=O)C(CCC(=O)O)NC(=O)C(NC(=O)C1CCCN1C(=O)C(N)Cc1ccc(O)cc1)C(C)C)C(=O)O, Question: What type of protein is the molecule a fragment of?, Option: ['casein', 'whey', 'lactalbumin', 'lactoferrin', 'albumin'].<|im_end|> \\n<|im_start|>system \\n\"\n",
    "inputs = tokenizer(sample_input, return_tensors=\"pt\")\n",
    "output = finetuned_model.generate(inputs[\"input_ids\"], max_length=500, num_return_sequences=1, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
